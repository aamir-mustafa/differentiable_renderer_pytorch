# -*- coding: utf-8 -*-
#"""background_and_color_space.ipynb
#
#Automatically generated by Colaboratory.
#
#Original file is located at
#    https://colab.research.google.com/github/BachiLi/redner/blob/master/tutorials/background_and_color_space.ipynb
#
#Redner's rendering functions can also output alpha channel. You can use this to composite rendered images with arbitrary background images.
#"""

#!pip install --upgrade redner-gpu

import torch
import pyredner
folder_name= 'HDR_Cube_3'
#"""We will download a famous test image in signal processing literature from the [SIPI image database](http://sipi.usc.edu/database/database.php?volume=misc&image=10#top).
#
#An important thing to keep in mind is that alpha blending is only *correct* in a [linear color space](https://www.kinematicsoup.com/news/2016/6/15/gamma-and-linear-space-what-they-are-how-they-differ). Natural 8-bit images you download from the internet is usually gamma compressed. Redner's `imread` function automatically converts the image to linear space (assuming gamma=2.2), so when displaying them you want to convert them back to the gamma compressed space.
#"""

# Commented out IPython magic to ensure Python compatibility.
#import urllib
#filedata = urllib.request.urlretrieve('http://sipi.usc.edu/database/download.php?vol=misc&img=4.2.03', 'mandrill.tiff')
#background = pyredner.imread('mandrill.tiff')
background = pyredner.imread('Target_Images_Cropped/background_cropped.exr')
# Visualize background
from matplotlib.pyplot import imshow
# %matplotlib inline
# Redner's imread automatically gamma decompress the image to linear space.
# You'll have to compress it back to sRGB space for display.
pyredner.imwrite(torch.pow(background.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/background.png') # saves an exr image as png

#imshow(torch.pow(background, 1.0/2.2))
# Convert background to current device
background = background.to(pyredner.get_device())

objects = pyredner.load_obj('ReferenceOutputMeshes/cubeNVO.obj', return_objects=True)
#camera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))

#"""Next, we define a `model` function that takes the objects, camera, and pose parameters, and output an image."""

# Obtain the teapot vertices we want to apply the transformation on.


#material_map2, mesh_list2, light_map2 = pyredner.load_obj('ReferenceOutputMeshes/cubeNVO.obj')
#for _, mesh2 in mesh_list2:
#    mesh2.normals = pyredner.compute_vertex_normal(mesh2.vertices/3, mesh2.indices)
#%%
#diffuse_reflectance_green =torch.tensor([0.65, 0.32, 0.16], device = pyredner.get_device())
#mat_green = pyredner.Material(diffuse_reflectance_green)
#materials = [mat_green]

#shape_cube = pyredner.Shape(vertices = mesh2.vertices/3,
#                               indices = mesh2.indices,
#                               uvs = None, normals = mesh2.normals, material_id = 0)


m = pyredner.Material(diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))

#sphere = pyredner.Object(vertices = mesh2.vertices/3, indices = mesh2.indices, uvs = None, 
#                         normals = mesh2.normals, material = m)

#cam = pyredner.automatic_camera_placement(shapes=[sphere], resolution=(background.shape[0], background.shape[1]))
cam = pyredner.Camera(position = torch.tensor([0.0, -0.0, -5.0]),  # -8.5
                      look_at = torch.tensor([0.0, 0.0, 0.0]),
                      up = torch.tensor([0.0, 1.0, 0.0]),
                      fov = torch.tensor([45.0]), # in degree
                      clip_near = 1e-2, # needs to > 0
                      resolution = (1650, 2843),
                      fisheye = False)

scene = pyredner.Scene(camera=cam, objects=objects)#[sphere])
lights = [pyredner.PointLight(cam.position.to(pyredner.get_device()),
                              torch.tensor([10.0, 10.0, 10.0], device = pyredner.get_device()))]
    
img = pyredner.render_albedo(scene)    
#img = pyredner.render_deferred(scene=scene, lights=lights, alpha=True)
imshow(torch.pow(img, 1.0/2.2).cpu())
pyredner.imwrite(torch.pow(img, 1.0/2.2).cpu(), 'results/'+folder_name+'/init_load.png')
#%%
vertices = []
for obj in objects:
    vertices.append(obj.vertices.clone())
# Compute the center of the teapot
center = torch.mean(torch.cat(vertices), 0)

def model(translation, euler_angles, scale):
    # Get the rotation matrix from Euler angles
    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)
    # Shift the vertices to the center, apply rotation matrix,
    # shift back to the original space, then apply the translation.
    for obj, v in zip(objects, vertices):
        obj.vertices = ((v - center)*scale) @ torch.t(rotation_matrix) + center + translation
    # Assemble the 3D scene.
    scene = pyredner.Scene(camera = cam, objects = objects)
    # Render the scene.
    img = pyredner.render_albedo(scene)
    return img
#%%



#target_translation = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device())
#target_euler_angles = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device())
#target_scale = torch.tensor([0.330], device = pyredner.get_device())
#target = model(target_translation, target_euler_angles, target_scale).data
#imshow(torch.pow(target, 1.0/2.2).cpu())


translation = torch.tensor([0.0, -0.0, 0.0], device = pyredner.get_device(), requires_grad=True)
euler_angles = torch.tensor([0.0, -0.0, 0.0], device = pyredner.get_device(), requires_grad=False)
scale = torch.tensor([0.330], device = pyredner.get_device(), requires_grad=True)
init = model(translation, euler_angles, scale)
# Visualize the initial guess
imshow(torch.pow(init.data, 1.0/2.2).cpu()) # add .data to stop PyTorch from complaining

pyredner.imwrite(torch.pow(init.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/init_transformed.png') # saves an exr image as png

##Loading it again

target = pyredner.imread('Target_Images_Cropped/cube_front_cropped.exr')
#
if pyredner.get_use_gpu():
    target = target.cuda()    
pyredner.imwrite(torch.pow(target.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/target.png') 

alpha = init[:, :, :]
blend_img = init[:, :, :] * alpha + background * (1 - alpha)
#imshow(torch.pow(blend_img.data, 1.0/2.2).cpu())
pyredner.imwrite(torch.pow(blend_img.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/blend_image.png') 

diff = torch.abs(target - blend_img)
pyredner.imwrite(torch.pow(diff.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/init_diff.png')


#%%

"""Next, we optimize the pose using Adam. We build two optimizers to have different learning rates for translation and rotation, since they have different dynamic ranges."""

t_optimizer = torch.optim.Adam([translation], lr=0.05)
r_optimizer = torch.optim.Adam([scale], lr=0.05)

# Commented out IPython magic to ensure Python compatibility.
# Setup plotting
#import matplotlib.pyplot as plt
# %matplotlib inline
#from IPython.display import display, clear_output
import time
t1= time.time()
#plt.figure()
imgs, losses = [], []
# Run 80 Adam iterations
num_iters = 1300
for t in range(num_iters):
    print('iteration:', t)
    t_optimizer.zero_grad()
    r_optimizer.zero_grad()
    img = model(translation, euler_angles, scale)
    
    alpha = img[:, :, :]
    blend_img = img[:, :, :] * alpha + background * (1 - alpha)
    # Compute the loss function. Here it is L2.
    # Both img and target are in linear color space, so no gamma correction is needed.
    loss = (blend_img - target).pow(2).mean()
    loss.backward()
    
    t_optimizer.step()
    r_optimizer.step()
    # Plot the loss
#    f, (ax_loss, ax_img) = plt.subplots(1, 2)
    
    losses.append(loss.data.item())
    imgs.append(torch.pow(img.data, 1.0/2.2).cpu()) # Record the Gamma corrected image
    if(t%10==0 or t==1299):
        pyredner.imwrite(torch.pow(blend_img.data, 1.0/2.2).cpu(), 'results/'+folder_name+'/iter_{}.png'.format(t))
#    clear_output(wait=True)
#    ax_loss.plot(range(len(losses)), losses, label='loss')
#    ax_loss.legend()
#    ax_img.imshow((img -target).pow(2).sum(axis=2).data.cpu())
#    plt.show()
    


#%%

#"""This time we'll use a simpler geometry -- a sphere. We can use generate_sphere to procedurally generate the triangle mesh geometry of a sphere."""
#
## The steps arguments decide how many triangles are used to represent the sphere.
#vertices, indices, uvs, normals = pyredner.generate_sphere(theta_steps = 64, phi_steps = 128)
#m = pyredner.Material(diffuse_reflectance = torch.tensor([0.5, 0.5, 0.5], device = pyredner.get_device()))
#sphere = pyredner.Object(vertices = vertices, indices = indices, uvs = uvs, normals = normals, material = m)
#cam = pyredner.automatic_camera_placement(shapes=[sphere], resolution=(background.shape[0], background.shape[1]))
#scene = pyredner.Scene(camera=cam, objects=[sphere])
#lights = [pyredner.PointLight(cam.position.to(pyredner.get_device()),
#                              torch.tensor([10.0, 10.0, 10.0], device = pyredner.get_device()))]
#img = pyredner.render_deferred(scene=scene, lights=lights, alpha=True)
#imshow(torch.pow(img, 1.0/2.2).cpu())
#
#"""Note that we set `alpha=True`. `img` is now a 4-channels image."""
#
#print(img.shape)
#
#"""We can alpha blend the image and the background like the following."""
#
#alpha = img[:, :, 3:4]
#blend_img = img[:, :, :3] * alpha + background * (1 - alpha)
#imshow(torch.pow(blend_img, 1.0/2.2).cpu())
#
#"""Note that it is not advisible to include `torch.pow(img, 1.0/2.2)` in the loss computation, since the derivative of `x^(1/2.2)` is negative infinite at `x=0`. There are two workarounds for this: first, you can invert the operation on the target (e.g., `torch.pow(target, 2.2)`). Second, you can use the [sRGB conversion](https://entropymine.com/imageworsener/srgbformula/), which uses a linear function near zero."""